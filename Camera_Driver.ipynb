{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=\"https://phi-35-smartgate.eastus.models.ai.azure.com\",\n",
    "    credential=AzureKeyCredential(\"AthQKaZDT3aGiaYYdfPykw5fTMyvKK2W\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=\"How many languages are in the world?\"),\n",
    "    ],\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "import base64\n",
    "\n",
    "image_url = \"https://news.microsoft.com/source/wp-content/uploads/2024/04/The-Phi-3-small-language-models-with-big-potential-1-1900x1069.jpg\"\n",
    "image_format = \"jpeg\"\n",
    "\n",
    "request = Request(image_url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "image_data = base64.b64encode(urlopen(request).read()).decode(\"utf-8\")\n",
    "data_url = f\"data:image/{image_format};base64,{image_data}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import TextContentItem, ImageContentItem, ImageUrl\n",
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content = [TextContentItem(text=\"You are a helpful assistant that can generate responses based on images.\")]),\n",
    "        UserMessage(content=[\n",
    "            TextContentItem(text=\"Which conclusion can be extracted from the following chart?\"),\n",
    "            ImageContentItem(image_url=ImageUrl(url=data_url))\n",
    "        ]),\n",
    "    ],\n",
    "    temperature=0,\n",
    "    top_p=1,\n",
    "    max_tokens=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env SPEECH_KEY=c0aaedf6bb204ddd85adc008a3e08749\n",
    "%set_env SPEECH_REGION=westus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import os\n",
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
    "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "# The neural multilingual voice can speak different languages based on the input text.\n",
    "speech_config.speech_synthesis_voice_name='en-US-AvaMultilingualNeural'\n",
    "\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "\n",
    "def say_it(text = \"Speech system test\"):\n",
    "#    pass\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "    \n",
    "    stream = speechsdk.AudioDataStream(speech_synthesis_result)\n",
    "\n",
    "    save_to = f\"no.wav\"\n",
    "    print(\"SaveTO: \", save_to)\n",
    "\n",
    "    stream.save_to_wav_file(save_to)\n",
    "    \n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        print(\"Speech synthesized for text [{}]\".format(text))\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "say_it(\"GPT says No\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "many_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
